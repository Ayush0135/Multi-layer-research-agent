# Automated Evidence-Based Research & Insight Engine ðŸ”¬ðŸ“š

**An autonomous AI agent designed to generate high-quality, Scopus-standard academic research papers through a rigorous multi-stage pipeline.**

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![Status](https://img.shields.io/badge/status-active-success)

## ðŸ“– Overview

The **Multi-Layer Research Agent** is a sophisticated system that emulates the workflow of a human researcher. Instead of a single LLM call, it orchestrates a specialized pipeline of agents to **decompose topics, search for real-world sources, analyze credentials, synthesize findings, and write professional academic papers**.

Key differentiator: It features a **Self-Correction Loop**. Stage 8 (Review) acts as a critical peer reviewer. If the generated paper does not meet academic standards (score < 7/10), it rejects the paper and sends constructive feedback back to Stage 7 for regeneration.

---

## ðŸš€ Key Features

*   **Multi-Stage Architecture**: Deconstructs the complexity of research into 8 manageable, specialized stages.
*   **Hybrid LLM Engine**:
    *   **Cloud Prioritized**: Uses **Gemini 1.5 Pro** and **Groq (Llama 3 70B)** for speed and high context.
    *   **Local Prioritized**: Fully capable of running completely locally using **Ollama** as a fallback or primary engine.
*   **Real-Time Internet Research**: Integrates with **Google Programmable Search Engine** to find recognized academic sources.
*   **Academic Scoring System**: Automatically evaluates sources based on relevance, credibility, and citationworthiness.
*   **Auto-Iterative Improvement**: The agent crtiques its own work and improves it iteratively before final output.

---

## ðŸ—ï¸ The Pipeline (8 Stages)

1.  **Topic Decomposition**: Breaks the user's prompt into key research questions and search queries.
2.  **Document Discovery**: Searches the web for PDFs, articles, and academic papers.
3.  **Analysis**: Reads contents, extracting key arguments, methodologies, and data.
4.  **Scoring**: Rates documents on a 0-10 academic scale; filters out low-quality/irrelevant noise.
5.  **Filtering & Selection**: Compiles the final "Knowledge Base" of top-tier references.
6.  **Synthesis**: Aggregates the knowledge base into a structured logical flow (outlining).
7.  **Generation**: Writes the full paper following Scopus/IEEE formatting standards.
8.  **Review**: A strict "Reviewer Agent" scores the paper. If the score is low, the cycle repeats.

---

## ðŸ› ï¸ Installation & Setup

### Prerequisites
*   Python 3.8+
*   (Optional) CUDA-enabled GPU for offline mode

### 1. Clone the Repository
```bash
git clone https://github.com/Ayush0135/Multi-layer-research-agent.git
cd Multi-layer-research-agent
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Configure Environment Variables
Create a `.env` file in the root directory:
```bash
cp .env.example .env
```
Edit `.env` and add your API keys:
```ini
GEMINI_API_KEY=your_gemini_key
GROQ_API_KEY=your_groq_key
GOOGLE_API_KEY=your_google_custom_search_api_key
GOOGLE_CSE_ID=your_search_engine_id
```

### 4. Setup Local Models (Ollama)
Ensure Ollama is installed from [ollama.com](https://ollama.com) and running. Then pull your preferred model:
```bash
python setup_offline.py
```
*(Defaults to llama3.2)*

---

## ðŸ–¥ï¸ Usage

Run the main script and provide a research topic:

```bash
python main.py "The Impact of Quantum Computing on Cryptography"
```

**Or simply run interactive mode:**
```bash
python main.py
```

The agent will print its progress through the stages. Upon success, the final paper will be saved as `paper_topic_name_paper.md`.

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py                # Entry point & orchestration logic
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # API Secrets (Ignored by Git)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ llm.py             # Interface for Cloud LLMs (Gemini/Groq)
â”‚   â”œâ”€â”€ llm_offline.py     # Interface for Local LLMs
â”‚   â””â”€â”€ search.py          # Google Search utilities
â”œâ”€â”€ stages/
â”‚   â”œâ”€â”€ stage1_topic.py      # Decomposition
â”‚   â”œâ”€â”€ stage2_discovery.py  # Search
â”‚   â”œâ”€â”€ stage3_analysis.py   # Content extraction
â”‚   â”œâ”€â”€ stage4_scoring.py    # Academic ranking
â”‚   â”œâ”€â”€ stage5_filtering.py  # Knowledge base selection
â”‚   â”œâ”€â”€ stage6_synthesis.py  # Logical structuring
â”‚   â”œâ”€â”€ stage7_generation.py # Drafting
â”‚   â””â”€â”€ stage8_review.py     # Quality control
â””â”€â”€ README.md
```

## âš ï¸ Note on Local Mode
Local mode uses **Ollama**. It is much faster and more memory-efficient than raw transformers for most users. Ensure you have the Ollama service running. You can configure the model in `.env` using `OLLAMA_MODEL=model_name`.

## ðŸ¤ Contribution
Contributions are welcome! Please fork the repo and submit a PR for any enhancements or bug fixes.

## ðŸ“„ License
This project is licensed under the MIT License.
